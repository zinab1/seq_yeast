{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Runs a ResNet model on the CIFAR-10 dataset.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import resnet_model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Basic model parameters.\n",
    "parser.add_argument('--data_dir', type=str, default='/tmp/cifar10_data',\n",
    "                    help='The path to the CIFAR-10 data directory.')\n",
    "\n",
    "parser.add_argument('--model_dir', type=str, default='/tmp/cifar10_model',\n",
    "                    help='The directory where the model will be stored.')\n",
    "\n",
    "parser.add_argument('--resnet_size', type=int, default=32,\n",
    "                    help='The size of the ResNet model to use.')\n",
    "\n",
    "parser.add_argument('--train_epochs', type=int, default=250,\n",
    "                    help='The number of epochs to train.')\n",
    "\n",
    "parser.add_argument('--epochs_per_eval', type=int, default=10,\n",
    "                    help='The number of epochs to run in between evaluations.')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help='The number of images per batch.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--data_format', type=str, default=None,\n",
    "    choices=['channels_first', 'channels_last'],\n",
    "    help='A flag to override the data format used in the model. channels_first '\n",
    "         'provides a performance boost on GPU but is not always compatible '\n",
    "         'with CPU. If left unspecified, the data format will be chosen '\n",
    "         'automatically based on whether TensorFlow was built for CPU or GPU.')\n",
    "\n",
    "_HEIGHT = 4\n",
    "_WIDTH = 50\n",
    "_DEPTH = 1\n",
    "_NUM_CLASSES = 1\n",
    "_NUM_DATA_FILES = 1\n",
    "\n",
    "location_of_s_bin = \"\"\n",
    "\n",
    "# We use a weight decay of 0.0002, which performs better than the 0.0001 that\n",
    "# was originally suggested.\n",
    "_WEIGHT_DECAY = 2e-4\n",
    "_MOMENTUM = 0.9\n",
    "\n",
    "_NUM_IMAGES = {\n",
    "    'train': 50000,\n",
    "    'validation': 10000,\n",
    "}\n",
    "\n",
    "\n",
    "def record_dataset(filenames):\n",
    "  \"\"\"Returns an input pipeline Dataset from `filenames`.\"\"\"\n",
    "  record_bytes = _HEIGHT * _WIDTH * _DEPTH + 1\n",
    "  return tf.data.FixedLengthRecordDataset(filenames, record_bytes)\n",
    "\n",
    "\n",
    "def get_filenames(is_training, data_dir):\n",
    "  \"\"\"Returns a list of filenames.\"\"\"\n",
    "  data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
    "\n",
    "  assert os.path.exists(data_dir), (\n",
    "      'Run cifar10_download_and_extract.py first to download and extract the '\n",
    "      'CIFAR-10 data.')\n",
    "\n",
    "  if is_training:\n",
    "    return [\n",
    "        os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "        for i in range(1, _NUM_DATA_FILES + 1)\n",
    "    ]\n",
    "  else:\n",
    "    return [os.path.join(data_dir, 'test_batch.bin')]\n",
    "\n",
    "\n",
    "def parse_record(raw_record):\n",
    "  \"\"\"Parse CIFAR-10 image and label from a raw record.\"\"\"\n",
    "  # Every record consists of a label followed by the image, with a fixed number\n",
    "  # of bytes for each.\n",
    "  label_bytes = 1\n",
    "  image_bytes = _HEIGHT * _WIDTH * _DEPTH\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "\n",
    "  # Convert bytes to a vector of uint8 that is record_bytes long.\n",
    "  record_vector = tf.decode_raw(raw_record, tf.uint8)\n",
    "\n",
    "  # The first byte represents the label, which we convert from uint8 to int32\n",
    "  # and then to one-hot.\n",
    "  label = tf.cast(record_vector[0], tf.int32)\n",
    "  label = tf.one_hot(label, _NUM_CLASSES)\n",
    "\n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(\n",
    "      record_vector[label_bytes:record_bytes], [_DEPTH, _HEIGHT, _WIDTH])\n",
    "\n",
    "  # Convert from [depth, height, width] to [height, width, depth], and cast as\n",
    "  # float32.\n",
    "  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def preprocess_image(image, is_training):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, _HEIGHT + 8, _WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [_HEIGHT, _WIDTH, _DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image\n",
    "\n",
    "\n",
    "def input_fn(is_training, data_dir, batch_size, num_epochs=1):\n",
    "  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n",
    "\n",
    "  Args:\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    data_dir: The directory containing the input data.\n",
    "    batch_size: The number of samples per batch.\n",
    "    num_epochs: The number of epochs to repeat the dataset.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of images and labels.\n",
    "  \"\"\"\n",
    "  dataset = record_dataset(get_filenames(is_training, data_dir))\n",
    "\n",
    "  if is_training:\n",
    "    # When choosing shuffle buffer sizes, larger sizes result in better\n",
    "    # randomness, while smaller sizes have better performance. Because CIFAR-10\n",
    "    # is a relatively small dataset, we choose to shuffle the full epoch.\n",
    "    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES['train'])\n",
    "\n",
    "  dataset = dataset.map(parse_record)\n",
    "  dataset = dataset.map(\n",
    "      lambda image, label: (preprocess_image(image, is_training), label))\n",
    "\n",
    "  dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "  # Batch results by up to batch_size, and then fetch the tuple from the\n",
    "  # iterator.\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  images, labels = iterator.get_next()\n",
    "\n",
    "  return images, labels\n",
    "\n",
    "\n",
    "def cifar10_model_fn(features, labels, mode, params):\n",
    "  \"\"\"Model function for CIFAR-10.\"\"\"\n",
    "  tf.summary.image('images', features, max_outputs=6)\n",
    "\n",
    "  network = resnet_model.cifar10_resnet_v2_generator(\n",
    "      params['resnet_size'], _NUM_CLASSES, params['data_format'])\n",
    "\n",
    "  inputs = tf.reshape(features, [-1, _HEIGHT, _WIDTH, _DEPTH])\n",
    "  logits = network(inputs, mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  predictions = {\n",
    "      'classes': tf.argmax(logits, axis=1),\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "  cross_entropy = tf.losses.mean_squared_error(\n",
    "      logits=logits, onehot_labels=labels)\n",
    "\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  # Add weight decay to the loss.\n",
    "  loss = cross_entropy + _WEIGHT_DECAY * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    # Scale the learning rate linearly with the batch size. When the batch size\n",
    "    # is 128, the learning rate should be 0.1.\n",
    "    initial_learning_rate = 0.1 * params['batch_size'] / 128\n",
    "    batches_per_epoch = _NUM_IMAGES['train'] / params['batch_size']\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n",
    "    boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n",
    "    values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n",
    "    learning_rate = tf.train.piecewise_constant(\n",
    "        tf.cast(global_step, tf.int32), boundaries, values)\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=_MOMENTUM)\n",
    "\n",
    "    # Batch norm requires update ops to be added as a dependency to the train_op\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(loss, global_step)\n",
    "  else:\n",
    "    train_op = None\n",
    "\n",
    "  accuracy = tf.metrics.accuracy(\n",
    "      tf.argmax(labels, axis=1), predictions['classes'])\n",
    "  metrics = {'accuracy': accuracy}\n",
    "\n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_accuracy')\n",
    "  tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=metrics)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  # Using the Winograd non-fused algorithms provides a small performance boost.\n",
    "  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "\n",
    "  # Set up a RunConfig to only save checkpoints once per training cycle.\n",
    "  run_config = tf.estimator.RunConfig().replace(save_checkpoints_secs=1e9)\n",
    "  cifar_classifier = tf.estimator.Estimator(\n",
    "      model_fn=cifar10_model_fn, model_dir=FLAGS.model_dir, config=run_config,\n",
    "      params={\n",
    "          'resnet_size': FLAGS.resnet_size,\n",
    "          'data_format': FLAGS.data_format,\n",
    "          'batch_size': FLAGS.batch_size,\n",
    "      })\n",
    "\n",
    "  for _ in range(FLAGS.train_epochs // FLAGS.epochs_per_eval):\n",
    "    tensors_to_log = {\n",
    "        'learning_rate': 'learning_rate',\n",
    "        'cross_entropy': 'cross_entropy',\n",
    "        'train_accuracy': 'train_accuracy'\n",
    "    }\n",
    "\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "        tensors=tensors_to_log, every_n_iter=100)\n",
    "\n",
    "    cifar_classifier.train(\n",
    "        input_fn=lambda: input_fn(\n",
    "            True, FLAGS.data_dir, FLAGS.batch_size, FLAGS.epochs_per_eval),\n",
    "        hooks=[logging_hook])\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_results = cifar_classifier.evaluate(\n",
    "        input_fn=lambda: input_fn(False, FLAGS.data_dir, FLAGS.batch_size))\n",
    "    print(eval_results)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNlab",
   "language": "python",
   "name": "nnlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
